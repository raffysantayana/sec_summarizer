{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360b5760-1ae7-489f-9d09-68dd2fe89d70",
   "metadata": {},
   "source": [
    "# Scraping the Data\n",
    "_Author_: https://github.com/raffysantayana\n",
    "\n",
    "## Goal\n",
    "Use the US Securities and Exchange Commision's (SEC) electronic filing system to programmatically parse and organize the data to later be explored, analyzed, and modeled.\n",
    "\n",
    "## Overview\n",
    "SEC archives quarterly reports from various filing entities such as Netflix Inc. (NFLX) and American Express Co. (AXP).\n",
    "\n",
    "WRITE SOMETHING HERE TALKING ABOUT THE API\n",
    "\n",
    "## Using the SEC API\n",
    "Note: This requires a subscription of $55/month to make 100+ requests. I wrote this code before realizing this, so this stopped after gathering a the free limit's worth of documents.\n",
    "```python\n",
    "import time\n",
    "import pandas as pd\n",
    "from sec_api import QueryApi\n",
    "\n",
    "# main dataframe we will append each query results to\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# paste your api key below\n",
    "sec_api_key:str = 'api_key' # 'a71896086f47a9ae5928bae84adfaff594ec0a1dcbb0bcc3db52ee3aa0f8e15c'\n",
    "query_api = QueryApi(api_key = sec_api_key)\n",
    "\n",
    "base_query = {\n",
    "  \"query\": \"PLACEHOLDER\", # this will be set during runtime \n",
    "  \"from\": \"0\",\n",
    "  \"size\": \"200\", # dont change this\n",
    "  # sort by filedAt\n",
    "  \"sort\": [{ \"filedAt\": { \"order\": \"desc\" } }]\n",
    "}\n",
    "\n",
    "# open the file we use to store the filing URLs\n",
    "log_file = open(\"filing_urls.txt\", \"a\")\n",
    "\n",
    "# start with filings filed in 2021, then 2020, 2019, ... up to 2010 \n",
    "# uncomment line below to fetch all filings filed in 2022-2010\n",
    "# for year in range(2021, 2009, -1):\n",
    "for year in range(2024, 1996, -1):\n",
    "    print(\"starting {year}\".format(year=year))\n",
    "    # a single search universe is represented as a month of the given year\n",
    "    \n",
    "    for month in range(1, 13, 1):\n",
    "        # get 10-Q and 10-Q/A filings filed in year and month\n",
    "        # resulting query example: \"formType:\\\"10-Q\\\" AND filedAt:[2021-01-01 TO 2021-01-31]\"\n",
    "        universe_query = \\\n",
    "            \"formType:\\\"10-Q\\\" AND \" + \\\n",
    "            \"filedAt:[{year}-{month:02d}-01 TO {year}-{month:02d}-31]\" \\\n",
    "            .format(year=year, month=month)\n",
    "    \n",
    "    print(universe_query)\n",
    "    # set new query universe for year-month combination\n",
    "    base_query[\"query\"] = universe_query;\n",
    "\n",
    "    # paginate through results by increasing \"from\" parameter \n",
    "    # until we don't find any matches anymore\n",
    "    # uncomment line below to fetch 10,000 filings\n",
    "    for from_batch in range(0, 999_800, 200): \n",
    "    # for from_batch in range(0, 400, 200):\n",
    "        # set new \"from\" starting position of search \n",
    "        base_query[\"from\"] = from_batch;\n",
    "\n",
    "        # submit request\n",
    "        response = query_api.get_filings(base_query)\n",
    "        # building a temp dataframe of the recent query\n",
    "        temp_df = pd.DataFrame.from_records(response['filings'])\n",
    "        # concatenating the temp dataframe to the main dataframe\n",
    "        df = pd.concat([df, temp_df])\n",
    "        print(f'df.shape = {df.shape}')\n",
    "        \n",
    "        # no more filings in search universe\n",
    "        if len(response[\"filings\"]) == 0:\n",
    "            break;\n",
    "            \n",
    "        # for each filing, only save the URL pointing to the filing itself \n",
    "        # and ignore all other data. \n",
    "        # the URL is set in the dict key \"linkToFilingDetails\"\n",
    "        urls_list = list(map(lambda x: x[\"linkToFilingDetails\"], response[\"filings\"]))\n",
    "        \n",
    "        # transform list of URLs into one string by joining all list elements\n",
    "        # and add a new-line character between each element.\n",
    "        urls_string = \"\\n\".join(urls_list) + \"\\n\"\n",
    "      \n",
    "        log_file.write(urls_string)\n",
    "\n",
    "log_file.close()\n",
    "```\n",
    "\n",
    "## Web Scraping\n",
    "SEC has an electrtonic filing system Electronic Data Gathering, Analysis, and Retrieval (EDGAR) that started around 1995 to archive reports such as quarterly 10Q. This system has a RESTful API at [this URL](https://www.sec.gov/edgar/sec-api-documentation) to retrieve report information. Each entityâ€™s current filing history is available at the following URL where CIK_number is an entity's 10 digit CIK number: `\n",
    "https://data.sec.gov/submissions/CIK{CIK_number}.json`\n",
    "\n",
    "The returning json contains information such as `accessionNumber`, and `primaryDocument` where the index of the `accessionNumber`is associated with the index of `primaryDocument`. Using these two pieces of info together with the CIK number allows us to construct a url to access all filings for that CIK. Our goal is to specifically analyze quarterly reports, so we will filter results based off of `form` value of \"10-Q\". The URL we will construct will be:\n",
    "`https://www.sec.gov/Archives/edgar/data/{CIK_number}/{accessionNumber}/{primaryDocument}`\n",
    "\n",
    "For example, https://www.sec.gov/Archives/edgar/data/0001445815/000149315224015525/form10-qa.htm\n",
    "\n",
    "A list of all CIK numbers to iterate through can be found [here](https://www.sec.gov/Archives/edgar/cik-lookup-data.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33f5ad3-f46f-4206-af13-9f55a1486f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c13a3d80-b15d-44e9-b0c1-a126a0b8ab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful response from https://www.sec.gov/files/company_tickers.json.\n",
      "10197 retrieved.\n"
     ]
    }
   ],
   "source": [
    "tickers_url:str = r'https://www.sec.gov/files/company_tickers.json'\n",
    "# Navigate to the target URL,\n",
    "# Inspect the page,\n",
    "# Network tab\n",
    "# Validate header values\n",
    "headers = {'User-Agent': r'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "          'Accept': r'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "          'Accept-Encoding': r'gzip, deflate, br, zstd',\n",
    "          'Accept-Language': r'en-US,en;q=0.9',\n",
    "          'Cache-Control': r'max-age=0',\n",
    "          'Cookie': r'bm_mi=BC7142F8086BAEE3F72651C3E83E0110~YAAQlvNuaPNr2zCSAQAAY1N8OhnmIHKjoxVwaI/ZtMIJX/PfxLczBtXiz0iklqZ+xIEpEcxpJU4cJMzVCwHoJuAFdGEhWaXyZk5X3zW+O+yIrIS8RiMBdXshd4jqETnc1qloYcASAXlWGbM5XtsAImK+Qai1DQrOUZcg25mQg1MnxRcGaFkmRkpFcYP0N7rikp5gu/ZDCZuzxAxxgc/2r95B2WELRm3i2NEuOmoatL799Y6yGKdJsdqkIwsxQyfPz8s+NYowETjxaAo9gTNVDD80EEJJqVzsIymXcUyE8g9TelOXLOEHp5RGVd0lCHQ0fXzFRs18a+SDa0nyUL/mfVP09Zzf36uhvaC4ZomMjhT8Hbi+C89fl2IZNCTck88Ssk7iPjAg~1; _ga=GA1.1.927723074.1727558080; nmstat=82418d68-15a7-185f-c020-d68d891b928b; ak_bmsc=90A1BDCD774101B03CE94836CAA9FBB6~000000000000000000000000000000~YAAQlvNuaAds2zCSAQAAb1d8OhkRpse9KikT1XMruvgV9301++feYbq5RTSLfeZbfESeyqDH+gyULs6M2CcqP2vLlcrkm50x+Mn2uuUJk6kkTcRrf0M9j4FKUBU85z/lpq3rM/OLwacdSqJ8MTbJS8+IJMxxRUKWDVu7zNArBxYwI/Y0UADubeL0p+/GNPNmVrrVBERf3xufgEwW+JdFU74FP/fCBvB0x7ntuiNcFCj3SbsYDhm4fd/+r/hJRmmQSMgn1yxV0hEQBYwxvtIrSPyIPkm/lB1SnYWBtJc4jeuI9t9Lwptbp5oxq5IbXhQkaAUqsJmbqsl7tvSNu7hfWkQfueVK1eWRVtEUMotCrxVKjbAeWRZCbTgKFQ+qr2bFQyW1ly8ChlbWNKuhet7EHI+cviLH2rQ+OrMLo7Q+06Dt035EyeQwTglPDqLvGC2JV8sQxuvGCFrX34o4LVx9FRSyBAcfvpiFYSz5mpGo21YPiim2FbNrQo2WSFfwbF3nalZUWSyFNcha3Djg44gCslDtf/8kUdZKWzc1ffQLgldX/oDVZDxAoQ==; _4c_=%7B%22_4c_s_%22%3A%22lZHNbsMgEIRfJeIcHMD495pzW7Wq2mPkwCa24gQLU9M08rt3sS2l6qnlAnzsjHaHG%2FE1XEjJM5ElSc5ywaVckxNce1LeiG102AZSErEXTFQqpRlPBJWsyOlei4oyiCsWZ6pK8wNZk8%2FgFSdxLFnOuCzGNVHd4nEjymhAL15EPI8YPfSocF9IqJAMz501%2BkO5nbt2oc7DftXrEz5oGBoFO99oV08Ggt1pDc2xdgELNuHOhguefHPRxv%2BWLfQuY7JAvLfG9xCk29qaM6y4CNhgEuR9koR2LRzA2qmsdq7ry83Gex8djTm2EClz3mBR37jQfw8KH4YFYKgzozN7awLVq9ftC%2FLHH%2BR5%2B%2FSwoG5YRmmNqtpgit81LjmLLJVSYs5Jhjm6lpR5KllY46ycYud%2FqZ6Hp6E7uPxHOo7f%22%7D; _ga_CSLL4ZEK4L=GS1.1.1727558080.1.0.1727558147.0.0.0; _ga_300V1CHKH1=GS1.1.1727558080.1.0.1727558162.0.0.0; bm_sv=0AA71C9F629AEC79558BCA20AC0B76BA~YAAQlvNuaP532zCSAQAAdbB9OhmiWaHQ21/biblHt4t4ehVBLbtki81OzkcMbg6vWn+G7Lm3XJqobk5BPFhd1InHg8eg68CJKtUXTOWFgHEdZlmYq2joAUvmemNV0qcAwNLWHd514rEAb2oV2awL97FYG2FuqnrpTRAVqLoE0TEy7fe9jDdbGKL4z2+NWF+ymu5Vw1A08NoOD19WVGezZdpy/qpZFiXyy6Y/ikKL0BW8YzmEHoYRBX8VBFPW~1'}\n",
    "response = requests.get(tickers_url, headers=headers)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"[ERROR] Status code {response.status_code} received\")\n",
    "else:\n",
    "    print(f\"Successful response from {tickers_url}.\")\n",
    "raw_tickers = response.json()\n",
    "print(f\"{len(raw_tickers)} retrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c944685-2f55-469e-bbf4-f91dd1e2930b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = pd.DataFrame(columns=['cik_str', 'ticker', 'title'])\n",
    "\n",
    "for i in range(len(raw_tickers)):\n",
    "    tickers.loc[f\"{i}\"] = raw_tickers[f\"{i}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd7ca78-ff5e-4f80-8b29-ceb1a3911f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10197, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a081442-90d0-4578-94d3-f188a0c3da61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cik_str     int64\n",
       "ticker     object\n",
       "title      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d63286-48bb-40fc-a4ab-72c4d2dbf539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik_str</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>320193</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>789019</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>MICROSOFT CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1045810</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>NVIDIA CORP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1652044</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1018724</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>AMAZON COM INC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cik_str ticker           title\n",
       "0   320193   AAPL      Apple Inc.\n",
       "1   789019   MSFT  MICROSOFT CORP\n",
       "2  1045810   NVDA     NVIDIA CORP\n",
       "3  1652044  GOOGL   Alphabet Inc.\n",
       "4  1018724   AMZN  AMAZON COM INC"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d7ddec-0f14-4f59-b2ca-2bf6690c5d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik_str</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10192</th>\n",
       "      <td>1849294</td>\n",
       "      <td>FRLAW</td>\n",
       "      <td>Fortune Rise Acquisition Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10193</th>\n",
       "      <td>886163</td>\n",
       "      <td>LGNDZ</td>\n",
       "      <td>LIGAND PHARMACEUTICALS INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10194</th>\n",
       "      <td>886163</td>\n",
       "      <td>LGNXZ</td>\n",
       "      <td>LIGAND PHARMACEUTICALS INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>886163</td>\n",
       "      <td>LGNYZ</td>\n",
       "      <td>LIGAND PHARMACEUTICALS INC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>886163</td>\n",
       "      <td>LGNZZ</td>\n",
       "      <td>LIGAND PHARMACEUTICALS INC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cik_str ticker                          title\n",
       "10192  1849294  FRLAW  Fortune Rise Acquisition Corp\n",
       "10193   886163  LGNDZ     LIGAND PHARMACEUTICALS INC\n",
       "10194   886163  LGNXZ     LIGAND PHARMACEUTICALS INC\n",
       "10195   886163  LGNYZ     LIGAND PHARMACEUTICALS INC\n",
       "10196   886163  LGNZZ     LIGAND PHARMACEUTICALS INC"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e7fe79-7e74-4de4-98f4-9e90437a368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers.to_csv(\"../data/tickers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce944e8e-c18d-4d29-bc01-6710d0a33107",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/all_submissions_w_duplicates.txt', 'w') as the_file:\n",
    "    for cik in tickers['cik_str']:\n",
    "        # Write to all_submissions.txt the url for the given cik number with\n",
    "        # leading zeroes until 10 digits are reached\n",
    "        the_file.write(f'https://data.sec.gov/submissions/CIK{cik:010d}.json\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4c249c7-ddb1-4738-9389-769e3d207f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/all_submissions_w_duplicates.txt', 'r') as dup_file:\n",
    "    lines = dup_file.readlines()\n",
    "    with open('../data//all_submissions.txt', 'w') as final_file:\n",
    "        for unique_line in set(lines):\n",
    "            final_file.write(unique_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca2806-a0ca-4552-9129-055b129eb952",
   "metadata": {},
   "source": [
    "## Validating URLs\n",
    "The cell below will iterate through each URL in `all_submissions.txt` and validate that they each provide a valid response. This only needs to be validated once assuming that SEC does not remove any of these submissions. With this assumption, the below code block will become markdown and will show the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed0b08-2d63-41c2-a446-91816637f786",
   "metadata": {},
   "source": [
    "```python\n",
    "urls = open('../data/all_submissions.txt', 'r')\n",
    "lines = urls.readlines()\n",
    "counter = 1\n",
    "for line in lines:\n",
    "    response = requests.get(line.strip(), headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f'{line.strip()} might not be a valid URL. Status code {response.status_code} received. This URL is on line {counter}.')\n",
    "        counter -= 1\n",
    "    # print the progress\n",
    "    print(f'{counter:05d}/{len(lines)}', end='\\r')\n",
    "    time.sleep(5)\n",
    "    counter += 1\n",
    "if counter - 1 == len(lines):\n",
    "    validation_file = open('../data/url_validation.txt', 'w')\n",
    "    validation_file.write('All URLs in all_submissions.txt have passed validation')\n",
    "```\n",
    "Output:\n",
    "10352/10352"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f94a81-6de2-49fa-86a4-34e8de1d0817",
   "metadata": {},
   "source": [
    "## Filtering URLs of CIK Submissions for 10Q Filings\n",
    "Now that we have URLs for all CIK numbers that detail all submissions that these entities have provided, we can move on to filter for the specific filing we want to train our model on. For this project, we will focus on 10Q quarterly reports. To show how we will filter for only 10Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b64c454-230e-4a43-8021-4be981924545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code 200 received for URL https://data.sec.gov/submissions/CIK0001978867.json\n"
     ]
    }
   ],
   "source": [
    "urls = open('../data/all_submissions.txt', 'r')\n",
    "lines = urls.readlines()\n",
    "\n",
    "# Get the 0th ticker\n",
    "response = requests.get(lines[0].strip(), headers=headers)\n",
    "print(f'Status code {response.status_code} received for URL {lines[0].strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82fd085a-d46b-4b61-9b49-680ef37fca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_json = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6746f55-ccfa-4518-83e7-abf20a9a3c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CDLR']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_json['tickers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9aadf91-7a3d-4d89-9140-e6927acbc7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NYSE']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_json['exchanges']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0bfb69-031c-4e4d-a51a-544bd68ba4a2",
   "metadata": {},
   "source": [
    "## Validating the number of values in each column\n",
    "A row in our dataframe will consist of the below values as well as the accession number above. These values will each be a column in the dataframe and there should be a value - non-null or null/empty string - for each submission for each column.\n",
    "\n",
    "### Accession Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d3416d4-4ad5-4a4e-b786-c372c8019f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "0001104659-24-090342\n"
     ]
    }
   ],
   "source": [
    "recent_filings = raw_json['filings']['recent']\n",
    "recent_filings['accessionNumber']\n",
    "print(len(recent_filings['accessionNumber']))\n",
    "print(recent_filings['accessionNumber'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d3a91-a4aa-4a71-8666-ba898be8efb8",
   "metadata": {},
   "source": [
    "### Filing Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c9cf565-a41c-420a-b13c-818e979b724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "2024-08-16\n"
     ]
    }
   ],
   "source": [
    "recent_filings['filingDate']\n",
    "print(len(recent_filings['filingDate']))\n",
    "print(recent_filings['filingDate'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065cfe3-966c-43d8-9b59-5b0d1264077b",
   "metadata": {},
   "source": [
    "### Report Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0b86bd9-3391-49b8-af63-bc09910330d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "2024-08-16\n"
     ]
    }
   ],
   "source": [
    "recent_filings['reportDate']\n",
    "print(len(recent_filings['reportDate']))\n",
    "print(recent_filings['reportDate'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac2f7d-ae30-4d7e-ba03-29b23a0d1da6",
   "metadata": {},
   "source": [
    "### Acceptance Date Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e52f1446-e470-4c48-a372-28e57071cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "2024-08-16T08:03:51.000Z\n"
     ]
    }
   ],
   "source": [
    "recent_filings['acceptanceDateTime']\n",
    "print(len(recent_filings['acceptanceDateTime']))\n",
    "print(recent_filings['acceptanceDateTime'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4828e6eb-79f8-4be1-8b4e-fcc9f6633543",
   "metadata": {},
   "source": [
    "### ACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c42c6cd6-7e8e-440f-be06-2435f5343733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "recent_filings['act']\n",
    "print(len(recent_filings['act']))\n",
    "print(recent_filings['act'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c06e92-809c-4522-8a8f-563c06875aec",
   "metadata": {},
   "source": [
    "### Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8753e10-1b72-4a2a-928c-8f808cd53193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "6-K\n"
     ]
    }
   ],
   "source": [
    "recent_filings['form']\n",
    "print(len(recent_filings['form']))\n",
    "print(recent_filings['form'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc430d-8b13-4289-b35c-a22f993a7468",
   "metadata": {},
   "source": [
    "### File Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37d32d97-a425-4280-8d82-25104509b48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "001-41889\n"
     ]
    }
   ],
   "source": [
    "recent_filings['fileNumber']\n",
    "print(len(recent_filings['fileNumber']))\n",
    "print(recent_filings['fileNumber'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4321b509-b11e-4d8f-bbea-f67df74ef7a2",
   "metadata": {},
   "source": [
    "### Film Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e36cbf8c-8f4c-42a6-9975-e0894053b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "241214172\n"
     ]
    }
   ],
   "source": [
    "recent_filings['filmNumber']\n",
    "print(len(recent_filings['filmNumber']))\n",
    "print(recent_filings['filmNumber'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44b609-ea0d-4391-afd0-ff9b302ac2b1",
   "metadata": {},
   "source": [
    "### Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "994bb9dd-3f9f-4d94-b6c9-bd8c921029ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recent_filings['items']\n",
    "print(len(recent_filings['items']))\n",
    "print(recent_filings['items'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05034040-9815-4b71-9c58-6bede7634777",
   "metadata": {},
   "source": [
    "### Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "738f4d81-e34e-48ee-a662-b71f72236b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "19362\n"
     ]
    }
   ],
   "source": [
    "recent_filings['size']\n",
    "print(len(recent_filings['size']))\n",
    "print(recent_filings['size'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cec10d-70d8-4b5b-a851-a0897e7a4d94",
   "metadata": {},
   "source": [
    "### Primary Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d987b6c6-c4fe-4a5d-b120-8e4827b4958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "tm2421902d1_6k.htm\n"
     ]
    }
   ],
   "source": [
    "recent_filings['primaryDocument']\n",
    "print(len(recent_filings['primaryDocument']))\n",
    "print(recent_filings['primaryDocument'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be706f7-7895-43e6-b675-7b5e0ce66556",
   "metadata": {},
   "source": [
    "### Is XBRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b9c2a73-e004-493f-96fb-bb6dbb6b10ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "recent_filings['isXBRL']\n",
    "print(len(recent_filings['isXBRL']))\n",
    "print(recent_filings['isXBRL'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece259e-6b15-4871-b39e-f1404273b905",
   "metadata": {},
   "source": [
    "### Is Inline XBRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b73d8428-755b-4ccd-b922-4b9564ad449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "recent_filings['isInlineXBRL']\n",
    "print(len(recent_filings['isInlineXBRL']))\n",
    "print(recent_filings['isInlineXBRL'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1905c7-2010-4bb1-b432-b5f99205f468",
   "metadata": {},
   "source": [
    "### Primary Doc Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49c0f075-659c-401d-bf12-0547a296d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "FORM 6-K\n"
     ]
    }
   ],
   "source": [
    "recent_filings['primaryDocDescription']\n",
    "print(len(recent_filings['primaryDocDescription']))\n",
    "print(recent_filings['primaryDocDescription'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d7365-54a8-4d39-a3cd-eefde822955d",
   "metadata": {},
   "source": [
    "`https://www.sec.gov/Archives/edgar/data/{CIK_number}/{accessionNumber}/{primaryDocument}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9b419-ef56-41b3-a51c-eb156f5a9498",
   "metadata": {},
   "source": [
    "### Function to Scrape Submission Metadata\n",
    "Submissions are grouped by CIK. URLs to the metadata for each group's submissions are detailed in URLs similar to the one below as JSONs where the digits in the URL are the CIK number with enough leading zeroes for a total of 10 digits.<br/>\n",
    "Example: https://data.sec.gov/submissions/CIK0001076682.json<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1158ad-b602-491c-a079-d5375ed80b6b",
   "metadata": {},
   "source": [
    "```python\n",
    "# Make a function that loops through the length of response.json()['filings']['recent'] and populates a list\n",
    "# whose items are in the order of the dataframe columns\n",
    "# start should start at which cik_index to start at (line in all_submissions.txt)\n",
    "def extract_10qs(cik_url_index=0):\n",
    "    line_counter = cik_url_index\n",
    "    tickers = []\n",
    "    exchanges = []\n",
    "    accession_numbers = []\n",
    "    filing_dates = []\n",
    "    report_dates = []\n",
    "    acceptance_datetimes = []\n",
    "    acts = []\n",
    "    forms = []\n",
    "    file_numbers = []\n",
    "    film_numbers = []\n",
    "    items = []\n",
    "    sizes = []\n",
    "    primary_documents = []\n",
    "    is_XBRLs = []\n",
    "    is_inline_XBRLs = []\n",
    "    primary_doc_descriptions = []\n",
    "    sources = []\n",
    "    has_multi_tickers = []\n",
    "    has_multi_exchanges = []\n",
    "    all_submissions_line_numbers = []\n",
    "    report_urls = []\n",
    "    \n",
    "    with open('../data/all_submissions.txt', 'r') as file_reader:\n",
    "        lines = file_reader.readlines()\n",
    "        line = lines[cik_url_index]\n",
    "        #for cik_index in range(len(lines)):\n",
    "        # for line in lines[start:]:\n",
    "            # Skip cik_index that is less than the specified starting index\n",
    "            # print(f'cik_index {cik_index} >= start {start} = {cik_index >= start}')\n",
    "            # if cik_index >= start:\n",
    "        print(f'Extracting reports from URL {line}', end='\\r')\n",
    "\n",
    "        # save cik number to build report url later\n",
    "        cik_number = line.split('/')[-1].split('.')[0][3:].strip('0')\n",
    "        \n",
    "        response = requests.get(line.strip(), headers=headers)\n",
    "\n",
    "        # WAIT 1 SECOND TO NOT DDOS THE GOVERNMENT\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f'Status code {response.status_code} received for URL {line.strip()}. URL on line {line_counter + 1}', end='\\r')\n",
    "        json = response.json()\n",
    "        curr_ticker = json['tickers']\n",
    "        curr_exchange = json['exchanges']\n",
    "        ticker_filings = json['filings']['recent']\n",
    "        for i_curr_ticker_filings in range(len(ticker_filings['accessionNumber'])):\n",
    "            if ticker_filings['form'][i_curr_ticker_filings] == '10-Q':\n",
    "                if len(curr_ticker) == 0:\n",
    "                    tickers.append(None)\n",
    "                else:\n",
    "                    tickers.append(curr_ticker[0])\n",
    "                if len(curr_exchange) == 0:\n",
    "                    exchanges.append(None)\n",
    "                else:\n",
    "                    exchanges.append(curr_exchange[0])\n",
    "                accession_numbers.append(ticker_filings['accessionNumber'][i_curr_ticker_filings])\n",
    "                filing_dates.append(ticker_filings['filingDate'][i_curr_ticker_filings])\n",
    "                report_dates.append(ticker_filings['reportDate'][i_curr_ticker_filings])\n",
    "                acceptance_datetimes.append(ticker_filings['acceptanceDateTime'][i_curr_ticker_filings])\n",
    "                acts.append(ticker_filings['act'][i_curr_ticker_filings])\n",
    "                forms.append(ticker_filings['form'][i_curr_ticker_filings])\n",
    "                file_numbers.append(ticker_filings['fileNumber'][i_curr_ticker_filings])\n",
    "                film_numbers.append(ticker_filings['filmNumber'][i_curr_ticker_filings])\n",
    "                items.append(ticker_filings['items'][i_curr_ticker_filings])\n",
    "                sizes.append(ticker_filings['size'][i_curr_ticker_filings])\n",
    "                primary_documents.append(ticker_filings['primaryDocument'][i_curr_ticker_filings])\n",
    "                is_XBRLs.append(ticker_filings['isXBRL'][i_curr_ticker_filings])\n",
    "                is_inline_XBRLs.append(ticker_filings['isInlineXBRL'][i_curr_ticker_filings])\n",
    "                primary_doc_descriptions.append(ticker_filings['primaryDocDescription'][i_curr_ticker_filings])\n",
    "                sources.append(line)\n",
    "                if len(curr_ticker) > 1:\n",
    "                    has_multi_tickers.append(1)\n",
    "                else:\n",
    "                    has_multi_tickers.append(0)\n",
    "                if len(curr_exchange) > 1:\n",
    "                    has_multi_exchanges.append(1)\n",
    "                else:\n",
    "                    has_multi_exchanges.append(0)\n",
    "                all_submissions_line_numbers.append(cik_url_index + 1)\n",
    "                report_urls.append(f'https://www.sec.gov/Archives/edgar/data/{cik_number}/{ticker_filings[\"accessionNumber\"][i_curr_ticker_filings].replace(\"-\", \"\")}/{ticker_filings[\"primaryDocument\"][i_curr_ticker_filings]}')\n",
    "    return pd.DataFrame({\n",
    "        'ticker': tickers,\n",
    "        'exchange': exchanges,\n",
    "        'accession_number': accession_numbers,\n",
    "        'filing_date': filing_dates,\n",
    "        'report_date': report_dates,\n",
    "        'acceptance_datetime': acceptance_datetimes,\n",
    "        'act': acts,\n",
    "        'form': forms,\n",
    "        'file_number': file_numbers,\n",
    "        'film_number': film_numbers,\n",
    "        'items': items,\n",
    "        'size': sizes,\n",
    "        'primary_document': primary_documents,\n",
    "        'is_XBRL': is_XBRLs,\n",
    "        'is_inline_XBRL': is_inline_XBRLs,\n",
    "        'primary_doc_description': primary_doc_descriptions,\n",
    "        'source': [source.strip() for source in sources],\n",
    "        'has_multi_ticker': has_multi_tickers,\n",
    "        'has_multi_exchange': has_multi_exchanges,\n",
    "        'all_submissions_line_number': all_submissions_line_numbers,\n",
    "        'report_url': report_urls\n",
    "    })\n",
    "\n",
    "functional_test_df = extract_10qs(cik_url_index=0)\n",
    "functional_test_df.shape\n",
    "```\n",
    "Output:<br/>Extracting reports from URL https://data.sec.gov/submissions/CIK0001686850.json</br>(19, 21)\n",
    "\n",
    "### Inspecting the Data Types\n",
    "\n",
    "```python\n",
    "functional_test_df.dtypes\n",
    "```\n",
    "\n",
    "Output:<br/>\n",
    "```\n",
    "ticker                         object\n",
    "exchange                       object\n",
    "accession_number               object\n",
    "filing_date                    object\n",
    "report_date                    object\n",
    "acceptance_datetime            object\n",
    "act                            object\n",
    "form                           object\n",
    "file_number                    object\n",
    "film_number                    object\n",
    "items                          object\n",
    "size                            int64\n",
    "primary_document               object\n",
    "is_XBRL                         int64\n",
    "is_inline_XBRL                  int64\n",
    "primary_doc_description        object\n",
    "source                         object\n",
    "has_multi_ticker                int64\n",
    "has_multi_exchange              int64\n",
    "all_submissions_line_number     int64\n",
    "report_url                     object\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "The `extract_10qs()` function can now be used to extract a maximum of `extraction_count` number of reports. It will return a dataframe of all 10q reports between the range of `start` and (`start` + `extraction_count`). The resulting dataframe can then be concatenated to the original dataframe. Gathering this metadata will take some time, so it is a huge benefit to periodically pause extraction, concatenate the incremental progress to the original dataframe, and then save the dataframe to a csv to continue progress at a later time.\n",
    "\n",
    "```python\n",
    "for i in functional_test_df.head().index:\n",
    "    print(functional_test_df.iloc[i][\"report_url\"])\n",
    "```\n",
    "Output:\n",
    "```\n",
    "https://www.sec.gov/Archives/edgar/data/168685/000149315224019257/form10-q.htm\n",
    "https://www.sec.gov/Archives/edgar/data/168685/000149315223040502/form10-q.htm\n",
    "https://www.sec.gov/Archives/edgar/data/168685/000149315223028452/form10-q.htm\n",
    "https://www.sec.gov/Archives/edgar/data/168685/000149315223016166/form10-q.htm\n",
    "https://www.sec.gov/Archives/edgar/data/168685/000149315222032096/form10-q.htm\n",
    "```\n",
    "\n",
    "### Gathering All 10-Q Reports\n",
    "\n",
    "The below block of code was executed to scrape the returned json's from `all_submissions.txt` to return a dataframe of only 10-Q reports. We will periodically save the csv that is being written in case of any interuptions in execution. Then we may check the `all_submissions_line_number` value of the latest written row and start the loop from that index. It is possible for human error to provide the incorrect index to resume work, so we will need to validate and clean the final result of the csv as needed.\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(columns=['ticker', 'exchange', 'accession_number', 'filing_date', 'report_date', 'acceptance_datetime', 'act', 'form', 'file_number', 'film_number', 'items', 'size', 'primary_document', 'is_XBRL', 'is_inline_XBRL', 'primary_doc_description', 'source', 'has_multi_ticker', 'has_multi_exchange', 'all_submissions_line_number', 'report_url'])\n",
    "                              \n",
    "with open('../data/all_submissions.txt', 'r') as submissions:\n",
    "    lines = submissions.readlines()\n",
    "    # Modify start of range as needed\n",
    "    for i_line in range(0, len(lines)):\n",
    "        df = pd.concat([df, extract_10qs(i_line)], axis=0)\n",
    "        df.to_csv('../data/debug/all_10qs.csv')\n",
    "```\n",
    "\n",
    "I had two interruptions due to my machine going to sleep while the above block ran. For example, the process ended at line number 7393, but I restarted the process at 7390. This is mostly because I didn't want to spend any time in that moment determining which 10-Q report URLs from the json on line 7393 have already been built, so I backtracked a few line indicies to be sure I have all the data I need at the cost of a few seconds of execution and cleaning up duplicates. I have manually moved the final `all_10qs.csv` out of the debug folder to the data folder. Let's do some of that cleanup in the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
