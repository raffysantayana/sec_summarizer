{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603938cf-413c-4327-b6be-79f5926c4dcf",
   "metadata": {},
   "source": [
    "# Summarize a Report\n",
    "_Author_: https://github.com/raffysantayana\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba29004-0694-4609-ac9d-8b5e5ea9cead",
   "metadata": {},
   "source": [
    "## LangChain and LangSmith Constants\n",
    "The LangChain library will be used to submit long prompts to LLMs. Each submission will contain multiple invocations of LLM calls. According to LangChain's tutorial documentation, LangSmith can be used to inspect what exactly is happening at each step of the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046d9ef-7096-489d-92c8-5d6ed038bc1f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dfa5c1f-a406-4182-8f90-c881a828df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "from langsmith import Client\n",
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826810b7-7444-4286-b0a1-c1bcff6967c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# Generate unique ID to help differentiate LangSmith logs\n",
    "unique_id = uuid4().hex[0:8]\n",
    "now = str(datetime.today()).replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "with open(f\"../langsmith_log_refs/summary_{now}.txt\", \"w+\") as log:\n",
    "    log.write(f\"Unique ID for runs starting at {now}: {unique_id}\")\n",
    "    now = None\n",
    "\n",
    "# Configuring the environment\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"sec10qs\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = r\"https://api.smith.langchain.com\"\n",
    "\n",
    "# create this directory structure and paste your keys into this file\n",
    "with open(f\"../../keys/langchain_key.txt\", \"r\") as langchain_key_reader:\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langchain_key_reader.read().strip()\n",
    "\n",
    "# create this directory structure and paste your keys into this file\n",
    "with open(f\"../../keys/openai_project_key.txt\", \"r\") as openai_key_reader:\n",
    "    foobar = openai_key_reader.read().strip()\n",
    "    print(foobar)\n",
    "    os.environ[\"OPENAI_API_KEY\"] = foobar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc5502-bc6c-44a3-8b1c-ed40f23d67ec",
   "metadata": {},
   "source": [
    "## LangChain Tutorial\n",
    "Following [LangChain's tutorial](https://python.langchain.com/docs/tutorials/llm_chain/#chaining-together-components-with-lcel) to learn how to use it to summarize a single report. Next notebook will apply what I've learned to summarize multiple reports in sequence.\n",
    "\n",
    "This also shows that the proper LangChain and OpenAI API keys are provided and enough funds are provided to OpenAI to submit requests. Below is the tutorial's initial prompt to translate English to Italian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84ceb02-8c1d-4f70-96e8-aea76156592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d13356-0c79-4890-92b1-7d065f1840e2",
   "metadata": {},
   "source": [
    "Using `model.invoke(messages)` returns back an `AIMessage`. This object contains the string response from OpenAI and other metadata. We can use LangChain's parser to extract only the string response. This can be done one of two ways.\n",
    "1. Use the model's `invoke` method passing in the messages followed by submitting the results of that request to the parser's `invoke` method.\n",
    "2. Chain the model's output to the parser using the `|` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0813515e-fbc5-4501-9ca3-cea8288472b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Ciao!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-3f37a545-7a4a-44f3-a4de-e1290ddf8e41-0' usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ciao!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "parser  = StrOutputParser()\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(result)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01cf5239-5e82-402c-8cb2-fa30214e3a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ciao!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "chain = model | parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93249bcc-7909-4b94-9628-dcd2beaff831",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "Prompt templates can be used to use application logic to transform user input into a list of messages for the LLM to process. Strings surrounded in `{}` brackets are variables that can be filled in using the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f78dd9-aad1-482f-8dd1-b598bc7c534f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following into italian:', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    # Tuples where the first string is \"system\"\n",
    "    # sets the stage for the upcoming chat\n",
    "    # This example informs the model the translate\n",
    "    # the following text into a given language\n",
    "    [(\"system\", system_template), \n",
    "     # tuples where the first string is \"user\"\n",
    "     # is the user input request\n",
    "     (\"user\", \"{text}\")]\n",
    "    # there is also the \"ai\" option which contains\n",
    "    # the LLM's preliminary response or follow-up\n",
    "    # question. For example,\n",
    "    # (\"ai\", \"I don't know how to translate that\")\n",
    ")\n",
    "\n",
    "result = prompt_template.invoke(\n",
    "    {\n",
    "        \"language\": \"italian\",\n",
    "        \"text\": \"hi\"\n",
    "    }\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd23fe-3865-491d-a53e-e964bf02ab35",
   "metadata": {},
   "source": [
    "Use the `invoke` method on the prompt template and pass in a dictionary of key-value pairs where the keys are the strings in the `{}` and the values are the strings that are being replaced with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e17489-62d3-4cba-9b13-d25a5e3b0f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into italian:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5852410-cfde-4910-a407-d54e20c852b9",
   "metadata": {},
   "source": [
    "## Chaining Together Components with LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c974ab-cf23-4762-867c-1bfd3c15d9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kamusta'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt_template | model | parser\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"language\": \"tagalog\",\n",
    "        \"text\": \"hi\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
